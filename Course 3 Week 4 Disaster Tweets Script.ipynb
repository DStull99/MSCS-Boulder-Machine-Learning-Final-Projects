{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to Github Repo: https://github.com/DStull99/MSCS-Boulder-Machine-Learning-Final-Projects\n",
    "\n",
    "# Brief Description of Problem, Relevant Features, and Strategy\n",
    "\n",
    "During real-world emergencies, people often tweet about ongoing disasters. The challenge is to classify tweets as disaster-related or not. This is difficult because many tweets use dramatic wording or sarcasm without referring to actual disasters (for example, \"this party is on fire\"). Our goal is to build a model that can distinguish real disaster tweets from unrelated ones. Dataset structure: Each tweet in the dataset has several fields​.\n",
    "\n",
    "id: unique identifier for the tweet\n",
    "text: the tweet content (actual tweet text)\n",
    "keyword: a keyword from the tweet (might be empty if no keyword provided)\n",
    "location: the location the tweet was sent from (could be blank)\n",
    "target: label indicating if the tweet is about a real disaster (1) or not (0)​\n",
    "\n",
    "The classification task is binary: predict target = 1 for disaster tweets and 0 for non-disaster tweets. This is important for emergency responders to filter signal from noise on Twitter​.\n",
    "\n",
    ". Correctly identifying disaster tweets (true positives) is crucial, and missing a real disaster tweet (false negative) could mean missing critical information. Therefore, we will prioritize recall (catching as many real disaster tweets as possible) while still aiming for good precision to avoid too many false alarms. \n",
    "\n",
    "Tweets are sequences of words, and understanding the context (word order and history) is key. RNNs are designed for sequential data and maintain a memory of previous words, which helps in understanding the tweet in context​.\n",
    "\n",
    "Unlike bag-of-words models, RNNs consider word order, meaning phrases like \"on fire\" can be interpreted with surrounding words. Specifically, we will experiment with two RNN variants: LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit). These networks include gating mechanisms that help them learn long-term dependencies in text. LSTMs have three gates (input, forget, output) and a cell state, whereas GRUs have a simpler design with two gates (reset and update) and no separate cell state​. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "\n",
      "Missing values per column:\n",
      "id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Loading & Inspecting the Data:\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "# Inspect first few rows\n",
    "print(df.head())\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, we can see there are multiple missing values in both the \"keyword\" column and \"location\" column. In order to handle these missing values, we will simply impute the word \"missing\" into the empty cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   keyword location\n",
      "0  missing  missing\n",
      "1  missing  missing\n",
      "2  missing  missing\n",
      "3  missing  missing\n",
      "4  missing  missing\n"
     ]
    }
   ],
   "source": [
    "# Imputing missing values:\n",
    "\n",
    "df[['keyword', 'location']] = df[['keyword', 'location']].fillna('missing')\n",
    "print(df[['keyword','location']].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    7613.00\n",
      "mean       14.90\n",
      "std         5.73\n",
      "min         1.00\n",
      "25%        11.00\n",
      "50%        15.00\n",
      "75%        19.00\n",
      "max        31.00\n",
      "Name: word_count, dtype: float64\n",
      "Tweet lengths (words) for each entry: [13, 7, 22, 8, 16, 18, 14, 15, 12, 10, 9, 27, 12, 7, 11, 3, 3, 3, 5, 3, 3, 4, 2, 4, 1, 6, 5, 3, 2, 4, 2, 5, 10, 9, 7, 13, 21, 8, 19, 5, 8, 11, 24, 6, 14, 16, 13, 10, 8, 27, 9, 11, 18, 10, 15, 19, 13, 22, 24, 16, 17, 18, 15, 23, 10, 8, 19, 27, 12, 16, 18, 14, 18, 4, 15, 12, 9, 9, 16, 18, 27, 19, 17, 14, 12, 17, 8, 24, 25, 6, 6, 8, 12, 9, 4, 26, 22, 21, 10, 22, 17, 20, 8, 20, 16, 11, 16, 8, 24, 11, 11, 11, 18, 2, 16, 16, 16, 9, 16, 16, 21, 16, 23, 8, 16, 19, 26, 16, 27, 11, 5, 2, 21, 19, 14, 18, 18, 20, 14, 18, 17, 20, 9, 10, 22, 10, 18, 20, 13, 13, 22, 18, 13, 10, 10, 11, 18, 11, 18, 18, 17, 25, 10, 14, 20, 5, 13, 21, 12, 7, 21, 22, 11, 20, 13, 9, 19, 10, 13, 13, 18, 13, 13, 11, 5, 11, 13, 8, 21, 14, 11, 10, 13, 13, 22, 12, 11, 11, 9, 9, 12, 11, 11, 11, 11, 11, 18, 11, 13, 17, 19, 6, 13, 19, 5, 16, 14, 17, 18, 4, 18, 13, 13, 17, 16, 22, 24, 17, 15, 8, 16, 9, 21, 14, 23, 13, 22, 20, 18, 15, 23, 22, 7, 17, 21, 6, 19, 4, 12, 8, 19, 19, 17, 19, 20, 6, 13, 21, 14, 19, 23, 22, 21, 18, 12, 23, 19, 9, 13, 12, 19, 19, 10, 16, 7, 16, 2, 17, 17, 20, 8, 19, 12, 5, 18, 10, 23, 18, 20, 13, 26, 16, 17, 16, 15, 17, 21, 14, 11, 14, 4, 21, 23, 19, 16, 16, 11, 5, 8, 11, 14, 12, 15, 16, 21, 22, 19, 16, 15, 16, 16, 2, 15, 2, 23, 14, 16, 20, 16, 16, 18, 15, 24, 9, 6, 20, 5, 16, 13, 9, 12, 18, 21, 24, 11, 12, 11, 12, 13, 10, 10, 12, 19, 18, 20, 14, 22, 8, 12, 18, 12, 11, 28, 14, 11, 12, 11, 11, 24, 20, 11, 10, 11, 10, 20, 22, 20, 20, 12, 11, 13, 15, 19, 6, 15, 17, 13, 13, 4, 13, 4, 11, 10, 13, 9, 19, 8, 19, 15, 18, 13, 4, 19, 13, 15, 13, 13, 11, 13, 14, 15, 11, 7, 9, 7, 16, 7, 11, 7, 4, 11, 4, 14, 14, 14, 17, 9, 12, 4, 14, 18, 6, 14, 2, 9, 9, 15, 11, 18, 5, 12, 8, 8, 20, 18, 16, 11, 12, 11, 7, 11, 20, 14, 16, 11, 13, 12, 9, 15, 13, 14, 14, 11, 19, 13, 27, 12, 18, 14, 7, 11, 21, 21, 20, 11, 24, 24, 8, 13, 12, 13, 3, 8, 18, 22, 4, 5, 20, 24, 12, 18, 18, 18, 17, 6, 18, 12, 4, 22, 18, 18, 21, 14, 15, 18, 18, 18, 17, 18, 12, 18, 18, 18, 25, 18, 9, 18, 13, 12, 6, 15, 16, 10, 21, 15, 15, 15, 12, 16, 19, 18, 9, 15, 15, 16, 10, 17, 17, 18, 11, 6, 14, 11, 15, 8, 22, 8, 18, 16, 13, 20, 14, 15, 21, 14, 21, 19, 21, 8, 12, 10, 12, 11, 17, 11, 4, 22, 9, 5, 18, 19, 13, 20, 15, 10, 16, 16, 9, 14, 16, 20, 17, 11, 16, 16, 19, 18, 15, 18, 20, 17, 15, 14, 20, 13, 15, 16, 10, 10, 17, 14, 16, 8, 14, 14, 11, 14, 17, 16, 15, 4, 20, 16, 18, 22, 20, 12, 14, 21, 15, 22, 21, 14, 20, 4, 11, 15, 16, 19, 20, 4, 17, 18, 22, 4, 22, 15, 20, 9, 15, 3, 8, 14, 26, 5, 5, 21, 14, 11, 15, 8, 8, 17, 12, 10, 3, 18, 19, 17, 10, 13, 24, 11, 5, 11, 20, 15, 10, 12, 14, 16, 14, 12, 21, 14, 9, 7, 14, 8, 11, 15, 19, 20, 20, 13, 18, 11, 20, 7, 12, 15, 14, 19, 15, 5, 19, 21, 21, 12, 3, 18, 6, 24, 17, 17, 5, 12, 20, 22, 23, 12, 17, 9, 12, 12, 14, 13, 20, 25, 15, 15, 15, 15, 7, 10, 11, 24, 5, 12, 11, 7, 13, 18, 7, 28, 8, 13, 9, 9, 14, 17, 18, 7, 5, 12, 12, 16, 8, 10, 13, 10, 20, 18, 27, 25, 21, 26, 17, 10, 9, 15, 23, 6, 8, 21, 24, 9, 26, 4, 17, 5, 5, 13, 19, 20, 15, 13, 5, 6, 23, 14, 23, 25, 2, 3, 26, 12, 11, 11, 12, 22, 10, 15, 10, 20, 13, 21, 11, 7, 5, 14, 12, 21, 9, 26, 19, 26, 24, 19, 20, 7, 4, 22, 7, 14, 6, 23, 21, 16, 24, 2, 16, 17, 2, 7, 8, 4, 10, 11, 22, 11, 7, 5, 8, 7, 12, 14, 4, 8, 6, 6, 7, 9, 9, 9, 10, 13, 28, 3, 4, 12, 13, 25, 18, 17, 19, 8, 19, 14, 25, 13, 14, 5, 17, 13, 18, 14, 5, 7, 11, 6, 18, 11, 11, 3, 7, 13, 19, 5, 13, 21, 6, 17, 20, 8, 7, 5, 18, 14, 9, 15, 11, 18, 11, 23, 5, 18, 18, 19, 26, 3, 5, 28, 18, 20, 26, 10, 23, 5, 8, 21, 5, 14, 17, 16, 17, 12, 17, 10, 22, 23, 24, 9, 5, 6, 22, 18, 9, 10, 15, 26, 18, 22, 17, 19, 17, 18, 20, 22, 25, 16, 14, 19, 17, 22, 28, 6, 11, 7, 28, 11, 20, 18, 18, 31, 25, 18, 17, 17, 11, 16, 20, 18, 13, 18, 28, 18, 18, 15, 16, 13, 18, 17, 18, 16, 13, 14, 16, 17, 20, 17, 16, 11, 18, 23, 20, 12, 6, 15, 18, 27, 4, 11, 7, 5, 5, 23, 14, 12, 19, 11, 15, 21, 14, 11, 21, 22, 25, 27, 20, 11, 21, 22, 13, 14, 6, 9, 12, 17, 19, 9, 15, 12, 9, 10, 14, 20, 3, 26, 21, 12, 23, 14, 13, 6, 10, 19, 22, 3, 16, 14, 14, 14, 8, 12, 13, 26, 10, 9, 8, 21, 15, 18, 13, 6, 18, 13, 19, 15, 20, 12, 19, 11, 10, 22, 6, 7, 15, 9, 9, 11, 6, 17, 14, 10, 11, 7, 22, 6, 9, 22, 29, 9, 6, 6, 20, 8, 17, 10, 16, 22, 25, 6, 24, 5, 19, 18, 10, 8, 22, 8, 22, 23, 19, 27, 4, 22, 15, 21, 4, 8, 14, 18, 9, 22, 20, 15, 16, 9, 16, 21, 8, 25, 11, 24, 8, 14, 13, 23, 7, 4, 14, 18, 12, 22, 22, 19, 20, 13, 16, 15, 10, 12, 8, 9, 9, 9, 9, 11, 11, 15, 9, 11, 20, 12, 8, 22, 11, 19, 10, 16, 18, 13, 18, 19, 11, 18, 22, 11, 14, 11, 15, 16, 11, 26, 15, 13, 21, 22, 11, 11, 14, 14, 21, 11, 11, 19, 13, 8, 23, 17, 11, 20, 16, 22, 12, 12, 11, 17, 21, 7, 10, 19, 23, 17, 21, 9, 17, 23, 11, 22, 5, 14, 16, 14, 18, 16, 19, 16, 11, 15, 23, 5, 9, 14, 23, 10, 17, 21, 19, 14, 16, 14, 13, 10, 9, 13, 23, 19, 19, 15, 16, 7, 18, 21, 23, 20, 17, 20, 19, 16, 7, 14, 20, 17, 21, 13, 20, 20, 15, 22, 19, 19, 25, 14, 15, 24, 27, 3, 6, 21, 22, 25, 24, 25, 11, 19, 13, 8, 10, 15, 10, 9, 26, 14, 16, 7, 22, 19, 20, 13, 14, 18, 20, 11, 21, 11, 16, 18, 17, 16, 20, 16, 18, 15, 8, 7, 9, 17, 18, 8, 13, 6, 17, 27, 4, 13, 13, 9, 9, 18, 14, 8, 13, 17, 14, 16, 17, 14, 27, 20, 11, 17, 27, 16, 19, 22, 21, 23, 16, 15, 20, 29, 21, 10, 5, 23, 19, 20, 23, 23, 11, 19, 13, 21, 19, 10, 17, 14, 9, 16, 16, 19, 22, 24, 14, 22, 8, 24, 27, 20, 17, 11, 14, 20, 13, 16, 16, 18, 5, 6, 24, 12, 22, 14, 17, 19, 18, 19, 17, 20, 8, 17, 19, 23, 10, 10, 11, 26, 15, 19, 20, 18, 11, 22, 5, 22, 13, 24, 22, 13, 13, 20, 24, 9, 19, 19, 17, 13, 16, 14, 7, 15, 18, 15, 14, 9, 12, 16, 20, 16, 17, 20, 17, 23, 16, 5, 18, 17, 11, 14, 11, 19, 19, 18, 15, 7, 23, 10, 11, 3, 13, 4, 11, 15, 7, 10, 22, 15, 8, 17, 13, 25, 13, 13, 20, 8, 22, 13, 19, 20, 16, 14, 3, 14, 18, 18, 18, 14, 14, 17, 13, 12, 9, 10, 27, 25, 13, 26, 24, 15, 13, 13, 23, 12, 19, 22, 9, 15, 17, 14, 14, 14, 14, 19, 11, 14, 9, 21, 14, 14, 19, 16, 19, 13, 11, 14, 15, 14, 13, 9, 18, 13, 24, 20, 10, 20, 11, 20, 13, 17, 12, 14, 19, 20, 19, 8, 18, 10, 19, 21, 11, 17, 18, 15, 18, 22, 12, 17, 18, 16, 17, 13, 12, 19, 22, 21, 15, 10, 16, 18, 10, 10, 21, 26, 24, 11, 13, 11, 12, 20, 12, 17, 9, 23, 6, 7, 19, 25, 14, 7, 18, 9, 12, 12, 13, 23, 23, 28, 10, 16, 23, 22, 23, 24, 21, 8, 9, 17, 18, 11, 19, 20, 11, 16, 19, 19, 17, 17, 12, 12, 14, 10, 13, 5, 10, 13, 24, 5, 28, 12, 13, 11, 16, 9, 12, 13, 9, 14, 6, 21, 18, 27, 21, 13, 15, 17, 19, 23, 8, 24, 19, 15, 7, 19, 20, 9, 17, 20, 21, 20, 26, 6, 15, 24, 21, 14, 13, 9, 11, 11, 11, 19, 20, 3, 19, 19, 7, 16, 19, 7, 14, 18, 19, 9, 8, 9, 11, 8, 17, 6, 16, 22, 18, 10, 12, 9, 27, 11, 12, 21, 14, 15, 20, 17, 11, 21, 7, 19, 15, 17, 24, 13, 19, 16, 17, 21, 17, 15, 17, 20, 22, 14, 9, 16, 13, 21, 23, 16, 22, 26, 27, 25, 4, 11, 16, 24, 28, 20, 22, 19, 17, 17, 22, 17, 16, 17, 19, 12, 18, 15, 11, 22, 19, 11, 18, 12, 6, 18, 12, 18, 25, 16, 10, 9, 12, 15, 14, 10, 6, 23, 9, 14, 19, 23, 18, 17, 12, 24, 14, 10, 16, 12, 9, 11, 14, 6, 20, 18, 11, 11, 17, 7, 14, 14, 9, 15, 10, 10, 16, 4, 5, 21, 7, 25, 5, 6, 20, 5, 20, 17, 7, 12, 7, 10, 8, 11, 13, 20, 10, 15, 12, 11, 13, 24, 18, 19, 27, 12, 19, 11, 18, 7, 17, 6, 19, 6, 6, 11, 15, 16, 29, 21, 15, 25, 6, 26, 21, 18, 24, 13, 15, 18, 18, 12, 18, 5, 8, 14, 20, 4, 12, 8, 4, 5, 6, 11, 11, 5, 5, 9, 11, 12, 9, 17, 23, 10, 12, 7, 8, 12, 5, 17, 5, 6, 5, 9, 17, 15, 6, 18, 5, 1, 7, 5, 6, 3, 12, 20, 3, 5, 8, 24, 6, 14, 11, 18, 13, 8, 17, 5, 7, 11, 9, 17, 9, 5, 14, 20, 22, 10, 9, 19, 9, 22, 5, 8, 27, 12, 19, 17, 7, 10, 7, 27, 13, 9, 18, 6, 10, 17, 15, 5, 25, 16, 26, 15, 10, 22, 27, 25, 27, 28, 9, 6, 19, 8, 30, 14, 12, 6, 21, 17, 22, 17, 21, 15, 12, 12, 18, 17, 9, 15, 17, 17, 20, 5, 12, 11, 17, 18, 15, 6, 18, 15, 15, 13, 9, 15, 19, 18, 17, 18, 10, 18, 16, 9, 5, 25, 8, 11, 13, 10, 12, 28, 7, 9, 19, 11, 27, 23, 19, 14, 16, 25, 18, 5, 13, 22, 15, 6, 13, 20, 10, 20, 18, 14, 17, 14, 7, 9, 4, 20, 23, 25, 5, 18, 15, 17, 15, 11, 22, 18, 17, 22, 12, 24, 11, 18, 7, 12, 14, 13, 15, 24, 12, 12, 10, 22, 15, 7, 14, 21, 23, 10, 17, 21, 28, 26, 8, 20, 19, 14, 18, 14, 19, 16, 6, 28, 17, 18, 15, 28, 8, 6, 26, 13, 28, 14, 13, 5, 28, 8, 12, 9, 7, 4, 18, 9, 30, 24, 8, 6, 11, 7, 16, 10, 12, 21, 4, 13, 24, 12, 16, 20, 11, 5, 4, 5, 13, 5, 8, 26, 15, 17, 6, 12, 22, 14, 8, 13, 13, 10, 7, 10, 22, 26, 17, 23, 6, 22, 22, 22, 18, 22, 21, 13, 17, 8, 21, 13, 3, 24, 21, 11, 3, 22, 12, 23, 7, 25, 19, 20, 16, 18, 20, 19, 9, 16, 21, 19, 21, 21, 22, 17, 18, 13, 6, 18, 14, 18, 15, 15, 15, 22, 18, 13, 15, 12, 18, 13, 14, 18, 11, 11, 10, 17, 9, 11, 14, 15, 14, 17, 21, 11, 15, 7, 20, 20, 14, 13, 15, 16, 23, 7, 24, 17, 7, 17, 12, 22, 19, 5, 13, 22, 23, 23, 15, 9, 12, 20, 16, 21, 22, 12, 14, 19, 21, 15, 7, 23, 17, 14, 24, 20, 23, 20, 10, 20, 22, 24, 20, 14, 15, 25, 9, 26, 23, 24, 24, 12, 24, 26, 12, 16, 11, 24, 17, 25, 24, 26, 18, 12, 25, 25, 18, 9, 25, 16, 21, 11, 15, 21, 10, 17, 22, 9, 8, 11, 11, 13, 20, 10, 9, 7, 14, 20, 17, 21, 8, 9, 18, 7, 10, 17, 17, 24, 24, 25, 20, 19, 23, 12, 11, 5, 13, 17, 15, 23, 8, 23, 19, 10, 20, 9, 14, 5, 20, 19, 13, 20, 17, 11, 20, 18, 7, 5, 9, 16, 9, 14, 11, 13, 15, 10, 11, 17, 9, 11, 21, 17, 16, 8, 14, 5, 19, 9, 6, 12, 6, 23, 16, 19, 22, 17, 9, 10, 18, 8, 14, 14, 12, 19, 14, 9, 16, 26, 12, 24, 26, 25, 17, 21, 14, 17, 26, 17, 14, 17, 14, 26, 8, 26, 21, 8, 17, 14, 15, 8, 11, 12, 17, 9, 21, 26, 11, 22, 11, 18, 14, 12, 15, 22, 17, 23, 21, 15, 17, 19, 7, 18, 14, 9, 25, 7, 21, 16, 24, 17, 24, 19, 25, 26, 7, 20, 17, 14, 23, 24, 19, 12, 15, 8, 13, 15, 20, 22, 18, 6, 9, 16, 9, 21, 13, 18, 11, 11, 20, 15, 9, 21, 11, 21, 21, 9, 17, 21, 21, 10, 18, 13, 17, 17, 11, 20, 15, 18, 12, 13, 11, 11, 17, 20, 25, 10, 15, 20, 9, 23, 18, 22, 11, 13, 15, 13, 24, 18, 10, 20, 22, 23, 6, 14, 17, 20, 22, 2, 11, 21, 17, 7, 20, 9, 18, 22, 10, 14, 11, 24, 14, 17, 20, 18, 12, 19, 14, 19, 20, 13, 16, 15, 14, 15, 17, 19, 18, 24, 27, 23, 14, 17, 17, 14, 21, 20, 14, 18, 9, 15, 6, 17, 24, 7, 15, 5, 20, 10, 13, 14, 19, 17, 12, 14, 15, 17, 10, 23, 13, 14, 23, 6, 14, 18, 22, 21, 21, 16, 12, 10, 11, 3, 8, 22, 24, 23, 10, 5, 23, 21, 25, 11, 14, 19, 23, 18, 4, 3, 15, 12, 20, 19, 19, 23, 19, 14, 14, 18, 7, 21, 14, 23, 16, 20, 25, 19, 16, 15, 19, 19, 16, 13, 19, 16, 18, 16, 8, 17, 16, 12, 18, 12, 12, 7, 9, 17, 21, 4, 10, 17, 17, 16, 10, 16, 7, 16, 6, 22, 20, 6, 5, 24, 16, 5, 10, 11, 8, 18, 18, 7, 7, 15, 18, 9, 7, 20, 18, 8, 11, 20, 19, 7, 9, 12, 6, 24, 8, 18, 13, 7, 7, 11, 9, 14, 24, 18, 8, 16, 11, 7, 18, 13, 16, 12, 9, 8, 8, 8, 9, 13, 22, 12, 11, 8, 9, 9, 8, 9, 9, 8, 17, 8, 11, 11, 20, 11, 11, 12, 11, 9, 19, 11, 15, 12, 8, 20, 19, 20, 11, 16, 15, 15, 8, 15, 20, 15, 20, 15, 5, 15, 19, 11, 16, 16, 14, 3, 10, 15, 8, 16, 7, 4, 3, 11, 23, 15, 10, 25, 19, 18, 19, 20, 20, 19, 19, 10, 18, 22, 19, 17, 19, 10, 20, 19, 17, 19, 20, 8, 19, 11, 17, 19, 19, 19, 16, 17, 19, 19, 18, 14, 22, 10, 9, 8, 4, 20, 19, 10, 24, 10, 3, 8, 6, 29, 17, 19, 10, 13, 17, 10, 14, 7, 9, 6, 23, 20, 10, 11, 26, 11, 23, 27, 10, 11, 24, 14, 15, 17, 22, 15, 15, 14, 24, 23, 18, 16, 23, 10, 15, 23, 15, 15, 15, 15, 15, 11, 11, 18, 17, 9, 13, 14, 17, 10, 17, 21, 13, 18, 13, 12, 18, 17, 14, 15, 6, 5, 17, 23, 7, 9, 12, 8, 23, 8, 20, 17, 18, 9, 19, 14, 16, 13, 8, 5, 26, 17, 18, 25, 13, 11, 14, 7, 11, 16, 7, 23, 18, 18, 26, 21, 18, 9, 24, 12, 14, 3, 16, 9, 15, 11, 9, 8, 19, 10, 21, 6, 10, 9, 8, 23, 13, 11, 25, 24, 17, 24, 23, 13, 10, 23, 21, 24, 7, 10, 25, 11, 16, 6, 13, 23, 18, 15, 13, 22, 14, 16, 11, 7, 9, 14, 9, 13, 26, 11, 25, 11, 11, 12, 17, 20, 23, 10, 15, 20, 4, 18, 16, 16, 5, 17, 17, 25, 9, 16, 12, 23, 5, 17, 6, 14, 5, 14, 12, 10, 16, 8, 14, 19, 21, 11, 18, 8, 17, 22, 5, 21, 25, 4, 14, 18, 9, 12, 26, 17, 12, 14, 12, 20, 19, 12, 7, 10, 17, 25, 12, 17, 17, 19, 11, 5, 14, 29, 17, 19, 16, 22, 7, 22, 20, 9, 19, 21, 23, 14, 24, 15, 20, 15, 16, 14, 14, 20, 16, 15, 17, 7, 14, 18, 17, 14, 20, 15, 17, 17, 21, 11, 6, 17, 11, 17, 17, 17, 16, 18, 9, 3, 16, 18, 18, 3, 20, 3, 12, 14, 18, 18, 15, 4, 9, 26, 8, 22, 21, 20, 9, 2, 4, 10, 7, 11, 10, 8, 26, 9, 16, 23, 17, 13, 5, 19, 6, 25, 13, 22, 13, 25, 6, 17, 13, 15, 15, 19, 10, 27, 11, 21, 7, 19, 10, 9, 12, 10, 16, 23, 17, 11, 25, 18, 29, 21, 18, 11, 23, 16, 17, 16, 7, 19, 22, 17, 22, 14, 8, 18, 18, 7, 10, 18, 2, 16, 17, 13, 17, 13, 15, 18, 16, 26, 17, 20, 18, 6, 15, 13, 15, 24, 17, 22, 15, 8, 11, 18, 16, 15, 18, 20, 20, 19, 17, 17, 6, 24, 17, 21, 19, 16, 17, 12, 12, 9, 24, 18, 11, 18, 12, 8, 11, 12, 21, 12, 8, 17, 21, 23, 17, 17, 16, 20, 18, 23, 12, 14, 10, 14, 16, 20, 21, 14, 18, 12, 18, 24, 11, 14, 20, 19, 12, 12, 15, 20, 15, 17, 18, 17, 22, 6, 17, 20, 19, 19, 18, 15, 15, 20, 20, 22, 17, 18, 12, 15, 15, 23, 22, 15, 23, 19, 20, 8, 13, 23, 12, 15, 23, 6, 12, 12, 8, 21, 21, 12, 23, 9, 23, 22, 15, 11, 12, 23, 15, 15, 15, 12, 16, 18, 17, 14, 12, 13, 8, 13, 14, 15, 8, 17, 10, 16, 27, 23, 22, 17, 24, 21, 13, 13, 8, 16, 19, 10, 16, 24, 7, 7, 12, 18, 15, 13, 23, 14, 14, 19, 10, 21, 24, 15, 26, 8, 9, 17, 17, 23, 4, 21, 21, 10, 15, 20, 11, 15, 15, 20, 18, 6, 13, 18, 13, 15, 17, 18, 14, 25, 16, 19, 18, 10, 14, 12, 13, 16, 10, 10, 22, 11, 3, 20, 10, 13, 15, 21, 17, 14, 23, 10, 20, 20, 11, 9, 20, 9, 11, 12, 19, 23, 12, 15, 27, 17, 11, 19, 21, 16, 16, 20, 18, 18, 19, 14, 12, 22, 14, 9, 22, 18, 8, 20, 21, 19, 13, 11, 10, 24, 12, 18, 7, 25, 18, 19, 10, 20, 24, 13, 15, 10, 15, 7, 19, 24, 21, 13, 5, 24, 5, 11, 24, 8, 20, 13, 20, 24, 14, 5, 24, 13, 5, 5, 11, 20, 13, 23, 19, 19, 6, 19, 10, 21, 29, 10, 10, 18, 14, 27, 21, 21, 10, 8, 6, 8, 17, 14, 24, 22, 5, 22, 21, 21, 8, 11, 16, 24, 24, 16, 14, 18, 10, 18, 12, 16, 11, 14, 23, 14, 13, 14, 16, 13, 14, 18, 14, 17, 16, 8, 16, 12, 11, 12, 16, 16, 18, 3, 17, 10, 26, 15, 16, 20, 10, 16, 20, 20, 14, 6, 6, 16, 15, 16, 21, 14, 15, 18, 20, 21, 17, 12, 12, 8, 13, 14, 16, 14, 18, 16, 16, 17, 14, 8, 18, 20, 16, 15, 16, 21, 16, 8, 9, 14, 15, 23, 8, 16, 10, 22, 19, 20, 9, 16, 20, 9, 20, 10, 17, 16, 12, 11, 9, 17, 12, 24, 11, 10, 20, 16, 10, 11, 8, 10, 17, 22, 20, 13, 13, 20, 10, 18, 11, 13, 10, 7, 14, 12, 11, 7, 20, 11, 20, 12, 16, 11, 12, 13, 20, 14, 19, 20, 9, 10, 20, 20, 21, 12, 10, 2, 20, 20, 10, 9, 20, 20, 10, 14, 21, 18, 13, 23, 17, 18, 24, 24, 12, 8, 15, 18, 8, 5, 19, 13, 19, 8, 8, 15, 13, 7, 9, 16, 19, 16, 11, 13, 19, 7, 16, 21, 9, 17, 11, 15, 13, 17, 16, 20, 14, 19, 22, 14, 7, 9, 4, 22, 21, 9, 1, 10, 13, 1, 14, 21, 11, 2, 8, 13, 12, 7, 11, 4, 2, 23, 3, 4, 13, 3, 11, 2, 11, 21, 19, 11, 18, 22, 18, 2, 17, 6, 12, 20, 14, 29, 11, 16, 20, 17, 7, 26, 17, 20, 9, 4, 20, 22, 21, 22, 4, 12, 13, 14, 7, 8, 22, 15, 18, 15, 23, 8, 19, 14, 7, 22, 26, 18, 17, 4, 17, 14, 14, 21, 9, 25, 7, 8, 6, 11, 8, 9, 3, 12, 4, 20, 12, 21, 12, 9, 6, 7, 15, 19, 6, 6, 18, 16, 20, 15, 15, 7, 14, 12, 17, 4, 12, 16, 12, 20, 24, 15, 8, 26, 21, 11, 22, 13, 18, 8, 9, 27, 12, 20, 9, 29, 22, 27, 20, 22, 16, 12, 11, 6, 8, 17, 8, 6, 16, 13, 16, 11, 14, 21, 10, 18, 19, 19, 13, 26, 18, 22, 17, 14, 12, 20, 11, 20, 15, 11, 13, 14, 15, 14, 16, 11, 20, 18, 20, 19, 14, 12, 17, 17, 11, 5, 14, 22, 4, 10, 11, 9, 10, 16, 11, 17, 21, 20, 11, 17, 6, 18, 30, 21, 7, 25, 20, 19, 10, 18, 6, 18, 19, 19, 21, 20, 17, 17, 19, 11, 7, 17, 18, 5, 8, 8, 2, 24, 22, 9, 22, 21, 20, 16, 3, 11, 5, 19, 20, 10, 4, 23, 19, 22, 22, 19, 3, 6, 24, 17, 19, 17, 25, 19, 17, 12, 20, 20, 7, 14, 20, 26, 19, 12, 20, 4, 20, 20, 10, 21, 16, 19, 21, 20, 15, 15, 20, 14, 17, 20, 20, 20, 12, 24, 20, 16, 13, 22, 20, 19, 10, 11, 24, 16, 15, 16, 3, 10, 16, 15, 9, 19, 25, 10, 15, 22, 15, 10, 10, 16, 16, 7, 19, 17, 15, 13, 9, 19, 9, 14, 18, 18, 8, 12, 17, 21, 24, 14, 23, 18, 21, 25, 25, 12, 20, 19, 6, 26, 22, 18, 20, 22, 15, 18, 13, 24, 8, 17, 11, 19, 17, 12, 21, 18, 14, 21, 23, 10, 14, 3, 16, 17, 23, 17, 17, 19, 21, 6, 10, 16, 18, 6, 15, 19, 11, 10, 28, 28, 21, 22, 9, 21, 8, 17, 28, 25, 16, 13, 22, 12, 15, 20, 15, 11, 16, 7, 14, 19, 14, 14, 20, 15, 21, 14, 11, 16, 13, 11, 16, 15, 16, 19, 25, 16, 14, 8, 11, 10, 20, 11, 24, 22, 21, 7, 15, 14, 24, 14, 18, 23, 12, 25, 5, 19, 7, 4, 18, 5, 5, 15, 11, 19, 23, 19, 6, 10, 4, 20, 22, 12, 20, 6, 17, 22, 16, 13, 8, 12, 9, 10, 6, 17, 9, 11, 14, 15, 11, 24, 15, 4, 14, 16, 10, 5, 19, 13, 19, 13, 20, 14, 13, 17, 17, 13, 21, 6, 20, 15, 23, 25, 14, 11, 20, 24, 17, 13, 28, 27, 19, 24, 5, 22, 13, 14, 15, 7, 22, 16, 16, 6, 12, 20, 24, 28, 19, 28, 11, 9, 18, 23, 18, 17, 12, 12, 20, 9, 7, 4, 18, 14, 11, 24, 24, 15, 6, 15, 13, 15, 18, 9, 16, 25, 21, 15, 16, 13, 12, 8, 10, 13, 15, 19, 15, 13, 24, 12, 13, 23, 19, 7, 7, 12, 8, 11, 14, 16, 9, 16, 16, 8, 15, 8, 21, 8, 21, 8, 28, 5, 8, 7, 11, 9, 14, 14, 7, 20, 9, 14, 16, 8, 9, 11, 28, 21, 24, 9, 11, 17, 21, 12, 17, 5, 18, 18, 22, 8, 13, 15, 25, 23, 15, 22, 19, 6, 15, 13, 13, 18, 7, 15, 20, 21, 12, 18, 19, 7, 20, 22, 16, 22, 17, 21, 7, 22, 21, 22, 21, 17, 26, 8, 11, 15, 22, 25, 16, 28, 21, 22, 16, 22, 17, 17, 22, 21, 18, 22, 16, 11, 5, 10, 22, 22, 18, 22, 13, 21, 20, 23, 15, 19, 10, 18, 14, 11, 13, 19, 24, 13, 18, 11, 12, 17, 19, 12, 12, 15, 4, 16, 12, 13, 3, 20, 21, 13, 12, 20, 20, 11, 16, 14, 9, 13, 9, 11, 8, 18, 9, 9, 9, 8, 7, 14, 9, 9, 9, 15, 8, 11, 9, 10, 8, 10, 12, 10, 12, 13, 18, 13, 10, 9, 13, 13, 8, 19, 16, 16, 16, 16, 13, 16, 16, 8, 16, 16, 10, 18, 16, 16, 16, 23, 16, 16, 10, 14, 14, 16, 26, 16, 16, 8, 10, 8, 16, 16, 16, 17, 16, 22, 15, 9, 5, 12, 19, 11, 9, 31, 11, 12, 17, 22, 11, 20, 11, 15, 23, 20, 20, 22, 11, 17, 11, 13, 18, 6, 17, 18, 19, 19, 16, 17, 21, 12, 10, 9, 19, 19, 18, 12, 17, 14, 20, 18, 19, 10, 18, 15, 25, 7, 19, 19, 17, 16, 18, 18, 15, 16, 18, 15, 19, 19, 24, 20, 12, 4, 16, 13, 8, 13, 5, 6, 9, 16, 9, 8, 14, 11, 4, 2, 12, 16, 12, 7, 6, 5, 9, 12, 16, 9, 15, 17, 21, 22, 13, 8, 16, 3, 14, 3, 25, 11, 18, 18, 10, 9, 14, 19, 9, 16, 13, 16, 10, 6, 18, 15, 7, 11, 24, 24, 9, 18, 17, 4, 9, 17, 18, 18, 18, 17, 21, 17, 17, 10, 18, 10, 17, 8, 13, 16, 24, 19, 20, 21, 19, 8, 23, 14, 20, 11, 19, 20, 11, 25, 15, 14, 22, 15, 15, 11, 9, 9, 21, 11, 23, 15, 13, 8, 10, 20, 16, 13, 13, 13, 12, 16, 16, 27, 16, 7, 13, 25, 15, 22, 21, 15, 13, 7, 13, 27, 18, 11, 10, 13, 14, 9, 7, 8, 13, 14, 26, 16, 10, 19, 16, 23, 13, 11, 19, 9, 18, 22, 22, 24, 19, 17, 13, 22, 13, 23, 22, 20, 21, 9, 22, 21, 14, 21, 18, 22, 27, 23, 5, 17, 18, 18, 26, 23, 18, 21, 8, 25, 26, 16, 24, 14, 11, 16, 16, 10, 9, 6, 8, 5, 16, 16, 21, 7, 22, 9, 22, 7, 15, 20, 17, 23, 10, 17, 8, 17, 17, 18, 24, 11, 23, 16, 16, 12, 24, 8, 8, 22, 26, 9, 8, 24, 10, 16, 9, 17, 15, 9, 25, 21, 18, 19, 10, 12, 15, 5, 5, 9, 25, 6, 19, 14, 4, 8, 20, 9, 3, 19, 7, 25, 25, 19, 6, 9, 16, 19, 10, 20, 14, 7, 14, 21, 20, 16, 13, 20, 10, 12, 16, 4, 14, 8, 13, 15, 12, 20, 14, 9, 7, 12, 21, 15, 9, 4, 22, 10, 15, 13, 20, 24, 24, 10, 7, 24, 24, 24, 24, 24, 26, 22, 24, 23, 27, 15, 24, 26, 14, 24, 24, 16, 24, 24, 23, 24, 28, 24, 11, 27, 22, 24, 16, 24, 18, 19, 17, 8, 27, 22, 18, 6, 22, 13, 14, 9, 16, 15, 22, 15, 18, 11, 20, 12, 20, 18, 23, 18, 22, 16, 7, 17, 17, 15, 14, 19, 24, 14, 5, 21, 22, 11, 23, 16, 22, 23, 25, 24, 9, 20, 17, 20, 13, 18, 5, 14, 11, 21, 15, 6, 24, 9, 11, 11, 10, 22, 4, 11, 6, 16, 21, 13, 11, 12, 13, 19, 14, 23, 16, 15, 12, 15, 18, 2, 4, 9, 15, 4, 20, 22, 18, 9, 11, 11, 24, 19, 13, 21, 9, 22, 22, 13, 13, 20, 17, 8, 17, 10, 14, 10, 11, 15, 13, 15, 18, 3, 7, 21, 21, 6, 14, 10, 17, 15, 9, 17, 18, 7, 21, 14, 11, 15, 16, 7, 11, 21, 26, 14, 6, 7, 14, 5, 17, 22, 9, 7, 25, 15, 15, 12, 23, 23, 26, 10, 15, 11, 14, 5, 11, 24, 15, 5, 6, 23, 1, 13, 11, 16, 13, 19, 15, 13, 19, 25, 17, 12, 11, 9, 11, 19, 17, 12, 18, 17, 13, 13, 17, 23, 19, 18, 13, 13, 18, 13, 13, 17, 12, 17, 31, 18, 17, 16, 23, 11, 12, 13, 12, 16, 6, 12, 17, 11, 17, 9, 9, 14, 11, 20, 11, 4, 25, 9, 9, 4, 18, 17, 19, 16, 3, 9, 5, 9, 6, 6, 5, 13, 10, 7, 18, 9, 12, 15, 21, 13, 12, 15, 20, 7, 10, 24, 19, 20, 10, 17, 22, 25, 14, 15, 19, 16, 15, 25, 22, 12, 8, 16, 14, 22, 17, 18, 18, 16, 10, 20, 15, 13, 9, 21, 15, 17, 15, 10, 11, 21, 11, 26, 11, 10, 21, 23, 18, 18, 11, 22, 16, 15, 10, 16, 15, 9, 22, 12, 11, 20, 27, 17, 14, 18, 1, 16, 10, 12, 12, 20, 17, 16, 21, 7, 18, 11, 17, 18, 20, 15, 9, 26, 13, 7, 19, 11, 6, 7, 18, 12, 14, 15, 13, 12, 15, 9, 17, 21, 10, 13, 22, 20, 18, 20, 11, 25, 21, 6, 19, 8, 14, 15, 13, 10, 12, 5, 12, 9, 18, 11, 20, 24, 25, 14, 13, 23, 13, 13, 24, 13, 21, 11, 8, 1, 13, 12, 5, 18, 18, 20, 16, 11, 14, 11, 20, 22, 20, 25, 5, 20, 11, 5, 13, 9, 10, 11, 13, 11, 19, 26, 10, 6, 21, 13, 14, 11, 13, 27, 15, 19, 23, 9, 5, 9, 16, 16, 7, 10, 9, 17, 20, 26, 26, 9, 15, 11, 10, 11, 22, 20, 12, 19, 10, 20, 13, 14, 11, 11, 11, 11, 11, 14, 18, 13, 21, 16, 19, 18, 11, 13, 17, 14, 17, 17, 18, 12, 6, 10, 11, 11, 12, 12, 13, 12, 21, 11, 13, 9, 14, 18, 22, 18, 10, 15, 20, 17, 18, 20, 11, 18, 18, 22, 18, 18, 18, 18, 19, 24, 18, 18, 9, 18, 23, 18, 18, 18, 18, 10, 16, 18, 13, 18, 19, 18, 18, 21, 18, 17, 16, 5, 12, 15, 20, 12, 24, 18, 17, 7, 13, 11, 3, 14, 11, 12, 11, 17, 12, 11, 13, 12, 16, 12, 15, 9, 12, 11, 12, 12, 12, 12, 10, 18, 8, 10, 20, 12, 3, 7, 13, 24, 27, 9, 11, 20, 7, 8, 28, 16, 19, 22, 13, 19, 13, 10, 19, 22, 14, 14, 28, 25, 20, 3, 15, 10, 17, 25, 22, 12, 16, 28, 18, 6, 6, 23, 11, 14, 9, 10, 14, 12, 5, 18, 20, 10, 13, 8, 13, 24, 18, 18, 14, 8, 14, 24, 7, 9, 21, 16, 11, 6, 20, 15, 8, 7, 11, 17, 22, 21, 20, 13, 16, 14, 15, 25, 12, 21, 19, 10, 13, 15, 16, 15, 16, 16, 17, 18, 19, 15, 6, 17, 11, 16, 18, 16, 14, 23, 15, 13, 26, 17, 10, 20, 19, 11, 7, 10, 11, 11, 13, 8, 22, 9, 14, 11, 9, 7, 6, 12, 17, 17, 9, 9, 11, 19, 8, 17, 17, 11, 17, 7, 7, 7, 11, 21, 7, 7, 7, 16, 29, 15, 16, 17, 9, 17, 14, 11, 15, 10, 17, 15, 9, 8, 12, 21, 13, 16, 15, 16, 19, 12, 26, 24, 10, 13, 17, 17, 15, 25, 11, 13, 16, 23, 16, 17, 9, 13, 11, 19, 21, 16, 17, 17, 9, 21, 9, 6, 16, 18, 13, 12, 16, 20, 11, 23, 20, 22, 15, 16, 19, 9, 14, 20, 14, 3, 20, 7, 9, 22, 23, 6, 26, 19, 19, 16, 8, 16, 11, 11, 13, 13, 18, 12, 13, 13, 16, 18, 13, 13, 12, 12, 12, 20, 12, 12, 13, 7, 13, 13, 12, 10, 13, 19, 14, 13, 13, 13, 11, 12, 13, 12, 13, 14, 19, 9, 16, 13, 12, 12, 19, 7, 9, 9, 19, 14, 8, 15, 20, 9, 23, 6, 11, 20, 15, 9, 9, 16, 22, 16, 17, 21, 7, 17, 16, 8, 20, 9, 24, 20, 8, 7, 12, 13, 18, 25, 12, 13, 7, 9, 12, 12, 21, 14, 11, 7, 13, 22, 10, 13, 14, 12, 25, 8, 12, 7, 29, 19, 11, 19, 18, 7, 7, 21, 6, 9, 11, 7, 16, 20, 24, 20, 20, 12, 10, 8, 13, 15, 11, 8, 11, 23, 26, 19, 16, 18, 7, 19, 18, 18, 18, 19, 20, 17, 10, 18, 21, 21, 12, 18, 18, 21, 19, 20, 21, 20, 18, 14, 21, 24, 18, 18, 18, 19, 20, 21, 14, 20, 9, 13, 20, 19, 21, 11, 5, 13, 15, 13, 12, 16, 18, 20, 7, 15, 17, 13, 14, 19, 16, 13, 14, 11, 21, 20, 16, 24, 16, 14, 21, 23, 16, 15, 11, 18, 13, 8, 19, 23, 16, 8, 17, 10, 16, 12, 19, 18, 8, 15, 16, 15, 22, 21, 20, 16, 20, 5, 5, 12, 23, 11, 13, 25, 20, 25, 21, 12, 13, 18, 8, 20, 23, 19, 20, 23, 16, 16, 25, 18, 13, 19, 13, 20, 21, 11, 13, 20, 12, 19, 14, 20, 7, 12, 13, 14, 13, 13, 20, 7, 13, 23, 14, 4, 8, 27, 8, 11, 13, 5, 23, 17, 18, 6, 17, 11, 11, 14, 15, 13, 20, 8, 20, 15, 10, 7, 21, 8, 18, 7, 11, 17, 12, 26, 14, 23, 8, 10, 11, 14, 14, 25, 14, 9, 14, 14, 14, 14, 14, 19, 3, 14, 14, 14, 14, 11, 16, 14, 14, 14, 14, 14, 12, 14, 14, 4, 14, 14, 14, 16, 14, 5, 10, 10, 14, 11, 15, 22, 11, 11, 23, 23, 20, 23, 7, 14, 14, 26, 19, 14, 20, 15, 20, 21, 27, 19, 23, 26, 11, 10, 7, 6, 24, 8, 15, 9, 5, 4, 24, 10, 16, 8, 26, 5, 5, 2, 5, 19, 4, 6, 9, 12, 20, 8, 3, 2, 10, 21, 18, 22, 16, 8, 8, 5, 11, 20, 3, 24, 3, 7, 4, 6, 9, 29, 5, 7, 3, 5, 20, 5, 2, 2, 26, 23, 17, 13, 7, 9, 5, 6, 14, 2, 14, 11, 9, 16, 15, 8, 30, 22, 12, 10, 23, 9, 23, 7, 25, 16, 3, 6, 5, 22, 12, 23, 9, 19, 14, 12, 15, 15, 18, 18, 12, 14, 13, 18, 16, 17, 18, 7, 20, 12, 10, 12, 8, 23, 25, 12, 11, 16, 20, 25, 14, 13, 9, 8, 10, 15, 9, 18, 11, 11, 22, 11, 15, 9, 14, 14, 7, 13, 9, 8, 8, 15, 5, 9, 18, 15, 17, 13, 9, 8, 19, 11, 11, 23, 6, 14, 15, 11, 14, 24, 24, 15, 25, 13, 21, 25, 16, 10, 25, 13, 12, 14, 8, 13, 25, 18, 13, 24, 17, 15, 17, 12, 9, 26, 25, 28, 16, 17, 22, 14, 25, 15, 5, 18, 25, 9, 14, 16, 11, 22, 3, 4, 11, 14, 4, 24, 22, 15, 11, 11, 18, 6, 27, 18, 25, 12, 9, 9, 14, 27, 20, 20, 11, 28, 9, 13, 12, 8, 5, 7, 24, 18, 26, 7, 12, 20, 14, 23, 11, 24, 14, 13, 4, 15, 10, 3, 6, 17, 21, 10, 17, 22, 3, 17, 19, 4, 12, 11, 19, 16, 15, 10, 13, 7, 15, 14, 11, 9, 10, 4, 9, 27, 20, 20, 12, 19, 5, 15, 7, 20, 16, 8, 5, 26, 11, 13, 20, 7, 9, 19, 17, 4, 8, 22, 14, 3, 14, 24, 17, 10, 9, 11, 28, 26, 19, 14, 10, 18, 13, 16, 15, 14, 8, 17, 15, 13, 10, 22, 14, 10, 27, 19, 17, 20, 14, 17, 28, 20, 22, 3, 18, 16, 13, 6, 17, 10, 24, 10, 21, 14, 18, 17, 12, 15, 11, 17, 8, 20, 2, 6, 15, 19, 11, 12, 4, 17, 10, 16, 14, 17, 13, 24, 11, 23, 17, 12, 15, 10, 24, 19, 14, 13, 7, 15, 7, 11, 9, 7, 12, 12, 4, 8, 9, 19, 7, 12, 14, 14, 18, 8, 15, 19, 26, 13, 14, 10, 8, 21, 10, 16, 11, 19, 18, 18, 12, 15, 14, 17, 14, 11, 18, 22, 14, 18, 18, 10, 18, 20, 20, 20, 19, 6, 17, 17, 18, 20, 23, 10, 18, 12, 15, 21, 22, 25, 16, 17, 17, 16, 16, 17, 16, 18, 23, 4, 16, 17, 18, 18, 16, 16, 18, 17, 21, 14, 14, 14, 17, 18, 16, 12, 18, 14, 18, 16, 20, 16, 18, 11, 11, 20, 10, 10, 8, 17, 17, 19, 12, 11, 18, 20, 6, 11, 23, 12, 16, 23, 13, 16, 19, 7, 11, 18, 22, 19, 19, 19, 11, 16, 23, 22, 13, 20, 18, 14, 9, 7, 17, 11, 9, 20, 19, 24, 6, 15, 22, 9, 9, 21, 17, 20, 7, 21, 8, 21, 15, 19, 9, 19, 22, 20, 14, 19, 15, 17, 17, 12, 18, 12, 25, 15, 12, 21, 14, 15, 22, 22, 7, 23, 12, 22, 24, 19, 24, 10, 11, 25, 9, 10, 27, 18, 21, 27, 20, 7, 11, 9, 23, 27, 11, 19, 14, 15, 18, 13, 15, 12, 16, 14, 14, 6, 10, 16, 15, 22, 5, 11, 21, 15, 15, 28, 11, 17, 17, 2, 24, 17, 7, 21, 9, 7, 11, 16, 19, 15, 22, 21, 27, 6, 11, 10, 28, 23, 21, 16, 12, 10, 13, 12, 18, 11, 18, 8, 15, 22, 20, 10, 20, 27, 21, 19, 17, 20, 14, 6, 13, 11, 8, 10, 18, 15, 18, 7, 18, 6, 10, 4, 21, 25, 15, 8, 14, 23, 13, 10, 21, 15, 22, 19, 13, 21, 20, 19, 6, 12, 4, 13, 10, 12, 13, 6, 10, 13, 16, 13, 15, 7, 17, 4, 12, 7, 8, 17, 14, 14, 12, 19, 14, 19, 6, 13, 19, 13, 26, 17, 13, 10, 10, 13, 12, 10, 3, 4, 27, 13, 17, 13, 16, 13, 13, 17, 18, 4, 8, 13, 17, 26, 12, 11, 21, 14, 13, 12, 12, 15, 6, 23, 21, 15, 12, 16, 17, 11, 23, 11, 18, 12, 23, 13, 17, 9, 12, 6, 6, 15, 4, 7, 13, 5, 17, 4, 23, 11, 6, 4, 12, 11, 11, 18, 16, 8, 13, 23, 15, 6, 16, 8, 8, 7, 3, 15, 7, 16, 25, 8, 19, 6, 1, 10, 17, 12, 11, 9, 17, 18, 12, 18, 21, 16, 9, 12, 21, 19, 17, 24, 18, 15, 20, 10, 18, 24, 12, 13, 17, 8, 15, 20, 10, 10, 18, 12, 5, 18, 16, 4, 12, 8, 6, 15, 13, 22, 20, 7, 18, 20, 12, 10, 26, 9, 11, 12, 21, 10, 15, 15, 26, 19, 18, 3, 12, 18, 9, 10, 10, 6, 15, 20, 17, 13, 14, 15, 14, 5, 19, 23, 17, 23, 12, 12, 17, 24, 13, 21, 11, 23, 19, 30, 22, 21, 21, 7, 23, 12, 23, 15, 23, 21, 26, 23, 23, 23, 20, 11, 4, 13, 6, 17, 11, 17, 25, 21, 10, 22, 9, 11, 19, 8, 14, 24, 17, 9, 17, 24, 9, 4, 19, 17, 12, 9, 17, 11, 20, 17, 17, 9, 9, 17, 17, 15, 21, 21, 22, 8, 9, 22, 21, 15, 12, 3, 9, 22, 20, 14, 14, 11, 12, 23, 10, 13, 23, 15, 25, 25, 12, 26, 15, 21, 20, 18, 14, 3, 15, 9, 12, 20, 25, 4, 5, 20, 11, 9, 4, 21, 6, 3, 15, 6, 20, 14, 20, 21, 21, 15, 25, 12, 21, 8, 14, 13, 5, 12, 15, 13, 12, 17, 17, 17, 16, 23, 3, 9, 9, 18, 13, 16, 7, 12, 12, 25, 9, 8, 9, 21, 17, 23, 18, 18, 13, 11, 16, 26, 11, 26, 9, 8, 5, 17, 20, 19, 11, 6, 12, 9, 16, 10, 19, 13, 12, 16, 4, 21, 11, 15, 5, 8, 22, 15, 4, 20, 17, 21, 16, 23, 16, 12, 6, 19, 4, 4, 13, 24, 16, 22, 25, 10, 11, 18, 9, 4, 9, 26, 8, 22, 10, 23, 14, 14, 10, 11, 16, 8, 3, 20, 18, 9, 12, 29, 10, 10, 13, 9, 7, 23, 6, 22, 12, 17, 20, 21, 13, 19, 20, 14, 15, 15, 15, 15, 22, 15, 16, 15, 14, 8, 13, 8, 22, 15, 15, 15, 6, 15, 9, 18, 18, 11, 9, 6, 8, 13, 19, 8, 15, 9, 6, 15, 15, 8, 21, 11, 19, 4, 11, 18, 16, 19, 18, 10, 22, 10, 22, 11, 11, 9, 22, 17, 8, 14, 13, 9, 9, 16, 12, 17, 22, 9, 21, 23, 11, 8, 11, 16, 17, 13, 11, 21, 18, 8, 9, 25, 16, 15, 16, 15, 16, 15, 15, 16, 16, 24, 11, 22, 11, 16, 24, 11, 9, 27, 14, 16, 12, 9, 22, 19, 14, 25, 9, 18, 16, 17, 19, 14, 18, 18, 14, 18, 18, 20, 17, 18, 18, 11, 17, 8, 20, 25, 17, 25, 4, 18, 7, 15, 6, 15, 18, 17, 22, 17, 17, 7, 19, 13, 12, 14, 10, 20, 10, 12, 24, 20, 18, 11, 27, 7, 16, 9, 19, 11, 15, 9, 23, 27, 23, 19, 16, 15, 4, 26, 3, 18, 20, 15, 14, 14, 16, 11, 13, 9, 8, 6, 22, 11, 19, 9, 7, 18, 17, 23, 8, 21, 18, 25, 7, 2, 8, 18, 27, 11, 8, 19, 20, 16, 16, 30, 21, 23, 13, 20, 17, 19, 18, 17, 9, 4, 20, 12, 22, 16, 18, 15, 26, 6, 17, 19, 18, 15, 17, 7, 17, 19, 18, 23, 22, 11, 19, 12, 16, 16, 24, 28, 8, 7, 19, 11, 26, 10, 16, 12, 20, 15, 23, 8, 6, 17, 13, 17, 18, 14, 19, 18, 17, 14, 10, 19, 8, 28, 3, 27, 17, 14, 16, 8, 22, 9, 23, 10, 17, 13, 13, 20, 20, 17, 21, 19, 9, 16, 12, 22, 15, 16, 20, 10, 17, 20, 26, 13, 7, 11, 20, 13, 22, 11, 14, 17, 7, 25, 14, 22, 18, 12, 13, 17, 17, 21, 11, 19, 17, 4, 13, 17, 15, 10, 12, 22, 13, 16, 15, 13, 12, 14, 17, 13, 17, 12, 8, 19, 20, 8, 17, 15, 10, 20, 11, 18, 12, 12, 12, 6, 10, 10, 13, 16, 10, 18, 8, 11, 12, 19, 16, 14, 11, 10, 11, 13, 7, 24, 15, 23, 18, 8, 8, 12, 12, 11, 18, 13, 23, 18, 13, 18, 12, 19, 9, 11, 19, 22, 17, 12, 7, 16, 23, 17, 18, 8, 19, 25, 12, 14, 7, 9, 23, 18, 10, 12, 11, 15, 16, 16, 17, 8, 20, 20, 17, 14, 21, 26, 11, 16, 23, 11, 10, 16, 14, 17, 21, 24, 11, 16, 22, 15, 8, 3, 25, 28, 13, 13, 14, 12, 22, 24, 16, 9, 13, 21, 13, 16, 25, 10, 21, 12, 22, 14, 4, 4, 24, 13, 16, 11, 7, 17, 14, 23, 14, 16, 17, 22, 19, 7, 25, 6, 15, 18, 8, 19, 12, 26, 13, 19, 6, 18, 10, 17, 28, 11, 22, 7, 13, 6, 17, 13, 17, 13, 17, 18, 17, 17, 28, 21, 9, 17, 17, 18, 17, 17, 8, 19, 13, 17, 25, 17, 21, 17, 17, 8, 12, 21, 8, 17, 11, 17, 8, 17, 17, 17, 17, 10, 17, 16, 22, 16, 20, 18, 10, 14, 10, 10, 12, 11, 10, 8, 16, 3, 8, 7, 5, 5, 5, 21, 9, 10, 2, 24, 16, 11, 12, 7, 11, 9, 7, 7, 21, 7, 20, 12, 13, 11, 17, 13, 8, 12, 2, 15, 2, 19, 16, 21, 12, 19, 14, 18, 18, 11, 18, 9, 13, 13, 20, 19, 26, 20, 16, 11, 20, 8, 19, 13]\n"
     ]
    }
   ],
   "source": [
    "# Examining the distributions of word counts for each tweet:\n",
    "\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "print(df['word_count'].describe().round(2))\n",
    "print(\"Tweet lengths (words) for each entry:\", df['word_count'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most common words (including stopwords): [('t', 5199), ('co', 4740), ('http', 4309), ('the', 3277), ('a', 2200)]\n"
     ]
    }
   ],
   "source": [
    "# Analyzing the most common words in these tweets:\n",
    "\n",
    "words = re.findall(r'\\w+', \" \".join(df['text']).lower())\n",
    "word_counts = Counter(words)\n",
    "print(\"Top 5 most common words (including stopwords):\", word_counts.most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, we're not getting much useful information from the most common words when common stopwords are included. We will have to clean the text data to remove these stopwords and other unnecessary text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Cleaned:  deed reason earthquak may allah forgiv us\n",
      "------\n",
      "Original: Forest fire near La Ronge Sask. Canada\n",
      "Cleaned:  forest fire near la rong sask canada\n",
      "------\n",
      "Original: All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "Cleaned:  resid ask shelter place notifi offic evacu shelter place order expect\n",
      "------\n",
      "Original: 13,000 people receive #wildfires evacuation orders in California \n",
      "Cleaned:  13 000 peopl receiv wildfir evacu order california\n",
      "------\n",
      "Original: Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
      "Cleaned:  got sent photo rubi alaska smoke wildfir pour school\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the text data to remove stopwords and other unnecessary text:\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text.lower())\n",
    "    words = text.split()\n",
    "    # Remove stopwords and single-character tokens\n",
    "    words = [w for w in words if w not in stop_words and len(w) > 1]\n",
    "    # Apply stemming\n",
    "    words = [ps.stem(w) for w in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing to all tweets\n",
    "df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Show original vs cleaned text for a few samples\n",
    "for i in range(5):\n",
    "    print(\"Original:\", df.loc[i, 'text'])\n",
    "    print(\"Cleaned: \", df.loc[i, 'text_clean'])\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the output above, the cleaned data does not contain stopwords or punctuation. The Porter Stemming algorithm does not provide a perfect result (we can see words like \"wildfir\", \"receiv\", \"earthquak\", etc.), but this result will suffice for this analysis. Next, we will transform the data using the tf-idf vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (7613, 18404)\n",
      "Sample TF-IDF features for first tweet:\n",
      "{'us': 0.28, 'forgiv': 0.46, 'allah': 0.41, 'may': 0.29, 'earthquak': 0.33, 'reason': 0.35, 'deed': 0.48}\n"
     ]
    }
   ],
   "source": [
    "# Transforming the data using the tf-idf vectorizer:\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['text_clean'])\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
    "# Show sample TF-IDF values for the first tweet\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "nz_indices = X_tfidf[0].nonzero()[1]\n",
    "print(\"Sample TF-IDF features for first tweet:\")\n",
    "print({feature_names[i]: round(X_tfidf[0, i], 2) for i in nz_indices})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has been cleaned and transformed, we will compare 2 different RNN implementations: LSTM and GRU. Firstly, however, we will tokenize the cleaned text and split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5709 tweets; Validation size: 1904 tweets\n",
      "Max sequence length: 25\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the cleaned text and splitting the data into training and validation sets:\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text_clean'])\n",
    "X_seq = tokenizer.texts_to_sequences(df['text_clean'])\n",
    "max_len = max(len(seq) for seq in X_seq)\n",
    "X_seq = pad_sequences(X_seq, maxlen=max_len, padding='post')\n",
    "y = df['target'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_seq, y, test_size=0.25, random_state=42)\n",
    "print(\"Train size:\", X_train.shape[0], \"tweets; Validation size:\", X_val.shape[0], \"tweets\")\n",
    "print(\"Max sequence length:\", max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture: \n",
    "\n",
    "For both LSTM and GRU models, we'll use a simple architecture: (1) An Embedding layer as the input layer to learn embeddings for each word index (we'll initialize it randomly and let the model learn), (2) One LSTM or GRU layer with a certain number of units (we'll start with 32 units for each, which is the dimensionality of the RNN's hidden state), (3) A Dropout within the RNN (we can use dropout and recurrent_dropout arguments) to help prevent overfitting, and (4) A Dense output layer with sigmoid activation to produce a probability for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "179/179 [==============================] - 4s 14ms/step - loss: 0.5639 - accuracy: 0.6982\n",
      "Epoch 2/10\n",
      "179/179 [==============================] - 5s 27ms/step - loss: 0.3067 - accuracy: 0.8800\n",
      "Epoch 3/10\n",
      "179/179 [==============================] - 3s 17ms/step - loss: 0.1688 - accuracy: 0.9394\n",
      "Epoch 4/10\n",
      "179/179 [==============================] - 3s 16ms/step - loss: 0.1040 - accuracy: 0.9681\n",
      "Epoch 5/10\n",
      "179/179 [==============================] - 3s 16ms/step - loss: 0.0606 - accuracy: 0.9813\n",
      "Epoch 6/10\n",
      "179/179 [==============================] - 3s 18ms/step - loss: 0.0428 - accuracy: 0.9898\n",
      "Epoch 7/10\n",
      "179/179 [==============================] - 3s 19ms/step - loss: 0.0306 - accuracy: 0.9940\n",
      "Epoch 8/10\n",
      "179/179 [==============================] - 3s 17ms/step - loss: 0.0279 - accuracy: 0.9940\n",
      "Epoch 9/10\n",
      "179/179 [==============================] - 3s 17ms/step - loss: 0.0285 - accuracy: 0.9947\n",
      "Epoch 10/10\n",
      "179/179 [==============================] - 3s 17ms/step - loss: 0.0249 - accuracy: 0.9953\n",
      "Epoch 1/10\n",
      "179/179 [==============================] - 4s 14ms/step - loss: 0.6844 - accuracy: 0.5659\n",
      "Epoch 2/10\n",
      "179/179 [==============================] - 3s 17ms/step - loss: 0.5633 - accuracy: 0.7012\n",
      "Epoch 3/10\n",
      "179/179 [==============================] - 3s 16ms/step - loss: 0.3188 - accuracy: 0.8767\n",
      "Epoch 4/10\n",
      "179/179 [==============================] - 3s 16ms/step - loss: 0.1831 - accuracy: 0.9357\n",
      "Epoch 5/10\n",
      "179/179 [==============================] - 3s 16ms/step - loss: 0.0988 - accuracy: 0.9706\n",
      "Epoch 6/10\n",
      "179/179 [==============================] - 3s 16ms/step - loss: 0.0518 - accuracy: 0.9855\n",
      "Epoch 7/10\n",
      "179/179 [==============================] - 3s 16ms/step - loss: 0.0373 - accuracy: 0.9898\n",
      "Epoch 8/10\n",
      "179/179 [==============================] - 3s 16ms/step - loss: 0.0272 - accuracy: 0.9946\n",
      "Epoch 9/10\n",
      "179/179 [==============================] - 3s 16ms/step - loss: 0.0236 - accuracy: 0.9947\n",
      "Epoch 10/10\n",
      "179/179 [==============================] - 3s 16ms/step - loss: 0.0204 - accuracy: 0.9956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c49bc60bb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training both the LSTM and GRU Models:\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embed_dim = 50  \n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_len),\n",
    "    LSTM(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "gru_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=max_len),\n",
    "    GRU(32, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train both models (using a small number of epochs for demonstration)\n",
    "lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "gru_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 0s 3ms/step\n",
      "60/60 [==============================] - 0s 2ms/step\n",
      "LSTM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.78      1091\n",
      "           1       0.70      0.69      0.70       813\n",
      "\n",
      "    accuracy                           0.74      1904\n",
      "   macro avg       0.74      0.74      0.74      1904\n",
      "weighted avg       0.74      0.74      0.74      1904\n",
      "\n",
      "LSTM Confusion Matrix:\n",
      "[[848 243]\n",
      " [249 564]]\n",
      "\n",
      "GRU Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.72      0.76      1091\n",
      "           1       0.67      0.77      0.72       813\n",
      "\n",
      "    accuracy                           0.74      1904\n",
      "   macro avg       0.74      0.75      0.74      1904\n",
      "weighted avg       0.75      0.74      0.74      1904\n",
      "\n",
      "GRU Confusion Matrix:\n",
      "[[787 304]\n",
      " [187 626]]\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on the validation set and comparing the classification results for the 2 models:\n",
    "\n",
    "y_pred_lstm = (lstm_model.predict(X_val) > 0.5).astype(int)\n",
    "y_pred_gru  = (gru_model.predict(X_val) > 0.5).astype(int)\n",
    "\n",
    "print(\"LSTM Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_lstm))\n",
    "print(\"LSTM Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_lstm))\n",
    "print(\"\\nGRU Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_gru))\n",
    "print(\"GRU Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred_gru))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, it appears that the GRU model performs slightly better than the LSTM, especially when it comes to recall (i.e., minimizing false negatives). As such, we will use the GRU model will to generate predictions to be used on the test set and submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# importing and preprocessing the test dataset:\n",
    "\n",
    "df2 = pd.read_csv('test.csv')\n",
    "df2[['keyword', 'location']] = df2[['keyword', 'location']].fillna('missing')\n",
    "df2['text_clean'] = df2['text'].apply(preprocess_text)\n",
    "X_tfidf = tfidf.transform(df2['text_clean'])\n",
    "\n",
    "X_seq_test = tokenizer.texts_to_sequences(df2['text_clean'])\n",
    "X_seq_test = pad_sequences(X_seq_test, maxlen=max_len, padding='post')\n",
    "\n",
    "y_pred_gru  = (gru_model.predict(X_seq_test) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Appending the predictions to the dataframe\n",
    "df2['target'] = y_pred_gru\n",
    "df2.drop(columns=['keyword', 'location', 'text', 'text_clean'], inplace=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving as a csv to submit:\n",
    "df2.to_csv('Disaster Tweets Submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
